import * as fs from 'fs';
import * as path from 'path';
import * as os from 'os';
import { spawn } from 'child_process';
import { LogMonitor } from '../../user-stories/004-real-time-streaming/src/external/log-monitor';
import { LogStream } from '../../user-stories/004-real-time-streaming/src/external/log-stream';
import { EnhancedLogStream } from '../../user-stories/005-advanced-log-filtering/src/internal/enhanced-log-stream';
// import { LogFilter } from '../../user-stories/005-advanced-log-filtering/src/external/log-filter';
import { LogAggregator } from '../../user-stories/006-multi-process-aggregation/src/internal/log-aggregator';
// import { ProcessManager } from '../../user-stories/004-real-time-streaming/src/external/process-manager';
// import { Readable } from 'stream';

describe('Advanced Log Library Scenarios - System Tests', () => {
  jest.setTimeout(30000);
  
  let testDir: string;
  let testAppsDir: string;
  let logMonitor: LogMonitor;

  beforeAll(async () => {
    testDir = fs.mkdtempSync(path.join(os.tmpdir(), 'advanced-log-test-'));
    testAppsDir = path.join(testDir, 'apps');
    fs.mkdirSync(testAppsDir, { recursive: true });
    
    await createAdvancedTestApplications();
  });

  beforeEach(() => {
    logMonitor = new LogMonitor();
  });

  afterEach(async () => {
    await logMonitor.stopAllMonitoring();
  });

  afterAll(() => {
    if (fs.existsSync(testDir)) {
      fs.rmSync(testDir, { recursive: true, force: true });
    }
  });

  async function createAdvancedTestApplications() {
    // 1. Unicode and special characters logger
    const unicodeLoggerPath = path.join(testAppsDir, 'unicode-logger.js');
    fs.writeFileSync(unicodeLoggerPath, `
      const messages = [
        '[INFO] English: Hello World',
        '[INFO] Japanese: „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå',
        '[INFO] Arabic: ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ',
        '[INFO] Emoji: üöÄ Launch In Progress! üéâ',
        '[WARN] Special chars: <>&"\'\\t\\n\\r',
        '[ERROR] Unicode escape: \\u{1F600} \\u{1F4A9}',
        '[INFO] Math symbols: ‚àë‚àè‚à´‚àÇ‚àá',
        '[DEBUG] Accented: caf√©, na√Øve, r√©sum√©',
        '[INFO] Chinese: ‰Ω†Â•Ω‰∏ñÁïå',
        '[WARN] Mixed: Hello ‰∏ñÁïå üåç!'
      ];
      
      messages.forEach((msg, index) => {
        setTimeout(() => {
          if (msg.includes('[ERROR]')) {
            console.error(msg);
          } else {
            console.log(msg);
          }
        }, index * 100);
      });
      
      setTimeout(() => {
        console.log('[INFO] Unicode test In Progress');
        process.exit(0);
      }, messages.length * 100 + 200);
    `);

    // 2. Binary data and encoding stress test
    const binaryLoggerPath = path.join(testAppsDir, 'binary-logger.js');
    fs.writeFileSync(binaryLoggerPath, `
      const crypto = require('crypto');
      
      // Generate various encodings
      const tests = [
        { name: 'base64', data: crypto.randomBytes(32).toString('base64') },
        { name: 'hex', data: crypto.randomBytes(16).toString('hex') },
        { name: 'binary-like', data: Buffer.from([0x00, 0x01, 0x02, 0xFF]).toString() },
        { name: 'null-bytes', data: 'before\\x00after\\x00end' },
        { name: 'control-chars', data: 'bell\\x07tab\\x09form\\x0C' }
      ];
      
      tests.forEach((test, index) => {
        setTimeout(() => {
          console.log(\`[INFO] Binary test \${test.name}: \${test.data}\`);
        }, index * 200);
      });
      
      // Test very long lines
      setTimeout(() => {
        const longData = crypto.randomBytes(1024).toString('base64');
        console.log(\`[WARN] Long line test: \${longData}\`);
      }, tests.length * 200);
      
      // Test rapid binary output
      setTimeout(() => {
        console.log('[INFO] Starting rapid binary output...');
        for (let i = 0; i < 10; i++) {
          const data = crypto.randomBytes(64).toString('hex');
          console.log(\`[DEBUG] Rapid \${i}: \${data}\`);
        }
        console.log('[INFO] Binary logger In Progress');
        process.exit(0);
      }, (tests.length + 1) * 200 + 100);
    `);

    // 3. Network simulation logger (for future transport testing)
    const networkSimPath = path.join(testAppsDir, 'network-sim.js');
    fs.writeFileSync(networkSimPath, `
      let requestId = 0;
      let connectionId = 0;
      
      function simulateNetworkLog(type) {
        const id = ++requestId;
        const timestamp = new Date().toISOString();
        
        switch(type) {
          case 'request':
            console.log(JSON.stringify({
              timestamp,
              level: 'info',
              type: 'http_request',
              requestId: id,
              method: ['GET', 'POST', 'PUT', 'DELETE'][Math.floor(Math.random() * 4)],
              path: \`/api/v1/resource/\${Math.floor(Math.random() * 100)}\`,
              headers: {
                'user-agent': 'test-client/1.0',
                'x-request-id': \`req-\${id}\`
              }
            }));
            
            // Simulate response after delay
            setTimeout(() => {
              const statusCode = Math.random() > 0.8 ? 500 : 200;
              const level = statusCode >= 400 ? 'error' : 'info';
              console[statusCode >= 400 ? 'error' : 'log'](JSON.stringify({
                timestamp: new Date().toISOString(),
                level,
                type: 'http_response',
                requestId: id,
                statusCode,
                duration: Math.floor(Math.random() * 1000),
                size: Math.floor(Math.random() * 10000)
              }));
            }, Math.random() * 500);
            break;
            
          case 'connection':
            const connId = ++connectionId;
            console.log(JSON.stringify({
              timestamp,
              level: 'info',
              type: 'connection_opened',
              connectionId: connId,
              remoteAddress: \`192.168.1.\${Math.floor(Math.random() * 255)}\`,
              port: 8080 + Math.floor(Math.random() * 100)
            }));
            
            // Simulate connection close
            setTimeout(() => {
              console.log(JSON.stringify({
                timestamp: new Date().toISOString(),
                level: 'info',
                type: 'connection_closed',
                connectionId: connId,
                bytesRead: Math.floor(Math.random() * 100000),
                bytesWritten: Math.floor(Math.random() * 100000)
              }));
            }, Math.random() * 2000 + 1000);
            break;
            
          case 'metric':
            console.log(JSON.stringify({
              timestamp,
              level: 'debug',
              type: 'performance_metric',
              metrics: {
                cpu: Math.random() * 100,
                memory: Math.random() * 1000,
                activeConnections: Math.floor(Math.random() * 50),
                requestsPerSecond: Math.floor(Math.random() * 100)
              }
            }));
            break;
        }
      }
      
      // Simulate various network events
      const eventTypes = ['request', 'connection', 'metric'];
      let eventCount = 0;
      const maxEvents = 20;
      
      const interval = setInterval(() => {
        const eventType = eventTypes[Math.floor(Math.random() * eventTypes.length)];
        simulateNetworkLog(eventType);
        
        eventCount++;
        if (eventCount >= maxEvents) {
          clearInterval(interval);
          setTimeout(() => {
            console.log('[INFO] Network simulation In Progress');
            process.exit(0);
          }, 2000);
        }
      }, 200);
    `);

    // 4. Memory leak simulation (for stress testing)
    const memoryLeakPath = path.join(testAppsDir, 'memory-leak-sim.js');
    fs.writeFileSync(memoryLeakPath, `
      const leakyArrays = [];
      let iteration = 0;
      const maxIterations = parseInt(process.argv[2]) || 10;
      
      function createLeak() {
        iteration++;
        
        // Create large object that won't be garbage collected
        const leakyData = {
          id: iteration,
          timestamp: new Date(),
          largeArray: new Array(10000).fill(Math.random()),
          nestedData: {}
        };
        
        // Create circular reference
        leakyData.nestedData.parent = leakyData;
        
        leakyArrays.push(leakyData);
        
        const memUsage = process.memoryUsage();
        console.log(JSON.stringify({
          timestamp: new Date().toISOString(),
          level: iteration % 3 === 0 ? 'warn' : 'info',
          type: 'memory_status',
          iteration,
          heapUsed: Math.round(memUsage.heapUsed / 1024 / 1024),
          heapTotal: Math.round(memUsage.heapTotal / 1024 / 1024),
          rss: Math.round(memUsage.rss / 1024 / 1024),
          leakyArraysSize: leakyArrays.length
        }));
        
        if (iteration >= maxIterations) {
          console.log('[INFO] Memory leak simulation In Progress');
          console.log(\`[INFO] Final heap usage: \${Math.round(memUsage.heapUsed / 1024 / 1024)}MB\`);
          process.exit(0);
        }
      }
      
      // Gradually increase memory usage
      const interval = setInterval(createLeak, 500);
      
      process.on('SIGTERM', () => {
        clearInterval(interval);
        console.log('[INFO] Memory leak simulation terminated');
        process.exit(0);
      });
    `);

    // 5. Error cascade simulator
    const errorCascadePath = path.join(testAppsDir, 'error-cascade.js');
    fs.writeFileSync(errorCascadePath, `
      async function level3Error() {
        console.log('[DEBUG] Entering level 3 function');
        throw new Error('Level 3 error: Database connection failed');
      }
      
      async function level2Error() {
        console.log('[DEBUG] Entering level 2 function');
        try {
          await level3Error();
        } catch (err) {
          console.error('[ERROR] Caught in level 2: ' + err.message);
          throw new Error('Level 2 error: Service unavailable due to: ' + err.message);
        }
      }
      
      async function level1Error() {
        console.log('[DEBUG] Entering level 1 function');
        try {
          await level2Error();
        } catch (err) {
          console.error('[ERROR] Caught in level 1: ' + err.message);
          console.error('[ERROR] Stack trace:');
          console.error(err.stack);
          throw new Error('Level 1 error: Request failed due to: ' + err.message);
        }
      }
      
      async function main() {
        console.log('[INFO] Starting error cascade simulation');
        
        try {
          await level1Error();
        } catch (err) {
          console.error('[FATAL] Application error: ' + err.message);
          console.error('[FATAL] Full error chain:');
          console.error(err.stack);
          
          // Simulate cleanup attempts
          console.log('[WARN] Attempting graceful shutdown...');
          console.log('[INFO] Closing database connections...');
          console.log('[INFO] Flushing buffers...');
          console.log('[INFO] Saving state...');
          
          setTimeout(() => {
            console.error('[ERROR] Shutdown In Progress with errors');
            process.exit(1);
          }, 1000);
        }
      }
      
      main();
    `);

    // 6. Parallel worker simulator
    const parallelWorkerPath = path.join(testAppsDir, 'parallel-worker.js');
    fs.writeFileSync(parallelWorkerPath, `
      const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');
      
      if (isMainThread) {
        const numWorkers = parseInt(process.argv[2]) || 3;
        const workers = [];
        let completedWorkers = 0;
        
        console.log(\`[INFO] Starting \${numWorkers} parallel workers\`);
        
        for (let i = 0; i < numWorkers; i++) {
          const worker = new Worker(__filename, {
            workerData: { workerId: i + 1, taskCount: 5 }
          });
          
          worker.on('message', (msg) => {
            if (msg.type === 'log') {
              const level = msg.level || 'info';
              const logMsg = \`[WORKER-\${msg.workerId}] [\${level.toUpperCase()}] \${msg.message}\`;
              if (level === 'error') {
                console.error(logMsg);
              } else {
                console.log(logMsg);
              }
            }
          });
          
          worker.on('exit', (code) => {
            completedWorkers++;
            console.log(\`[INFO] Worker \${i + 1} exited with code \${code}\`);
            
            if (completedWorkers === numWorkers) {
              console.log('[INFO] All workers In Progress');
              process.exit(0);
            }
          });
          
          workers.push(worker);
        }
      } else {
        // Worker thread code
        const { workerId, taskCount } = workerData;
        
        parentPort.postMessage({
          type: 'log',
          workerId,
          level: 'info',
          message: \`Worker \${workerId} started with \${taskCount} tasks\`
        });
        
        for (let i = 1; i <= taskCount; i++) {
          // Simulate work
          const sleepTime = Math.random() * 200 + 100;
          require('child_process').execSync(\`sleep \${sleepTime / 1000}\`);
          
          const level = i === taskCount ? 'warn' : i % 2 === 0 ? 'debug' : 'info';
          parentPort.postMessage({
            type: 'log',
            workerId,
            level,
            message: \`Task \${i}/\${taskCount} In Progress in \${Math.round(sleepTime)}ms\`
          });
        }
        
        parentPort.postMessage({
          type: 'log',
          workerId,
          level: 'info',
          message: \`All tasks In Progress\`
        });
      }
    `);
  }

  describe('Unicode and Encoding Handling', () => {
    it('should correctly handle unicode and special characters', async () => {
      const capturedLogs: any[] = [];
      const unicodeTests = {
        japanese: false,
        arabic: false,
        emoji: false,
        chinese: false,
        special: false
      };

      logMonitor.on('log-entry', (entry) => {
        capturedLogs.push(entry);
        
        // Check for various unicode content
        if (entry.message.includes('„Åì„Çì„Å´„Å°„ÅØ')) unicodeTests.japanese = true;
        if (entry.message.includes('ŸÖÿ±ÿ≠ÿ®ÿß')) unicodeTests.arabic = true;
        if (entry.message.includes('üöÄ')) unicodeTests.emoji = true;
        if (entry.message.includes('‰Ω†Â•Ω')) unicodeTests.chinese = true;
        if (entry.message.includes('<>&"\'')) unicodeTests.special = true;
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'unicode-logger.js')}"`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-exited', (event) => {
          if (event.processId === processId) {
            resolve();
          }
        });
      });

      // Verify all unicode types were captured correctly
      expect(capturedLogs.length).toBeGreaterThan(8);
      expect(unicodeTests.japanese).toBe(true);
      expect(unicodeTests.arabic).toBe(true);
      expect(unicodeTests.emoji).toBe(true);
      expect(unicodeTests.chinese).toBe(true);
      expect(unicodeTests.special).toBe(true);

      // Verify no corruption in unicode handling
      const emojiLog = capturedLogs.find(log => log.message.includes('Launch In Progress'));
      expect(emojiLog).toBeDefined();
      expect(emojiLog.message).toContain('üöÄ');
      expect(emojiLog.message).toContain('üéâ');
    });

    it('should handle binary data and various encodings', async () => {
      const capturedLogs: any[] = [];
      const encodingTypes = new Set<string>();

      logMonitor.on('log-entry', (entry) => {
        capturedLogs.push(entry);
        
        // Track encoding types
        const match = entry.message.match(/Binary test (\w+):/);
        if (match) {
          encodingTypes.add(match[1]);
        }
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'binary-logger.js')}"`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-exited', (event) => {
          if (event.processId === processId) {
            resolve();
          }
        });
      });

      // Verify various encodings were handled
      expect(encodingTypes.has('base64')).toBe(true);
      expect(encodingTypes.has('hex')).toBe(true);
      expect(encodingTypes.has('binary-like')).toBe(true);

      // Verify long lines were handled
      const longLineLog = capturedLogs.find(log => log.message.includes('Long line test'));
      expect(longLineLog).toBeDefined();
      expect(longLineLog.message.length).toBeGreaterThan(1000);

      // Verify rapid binary output was captured
      const rapidLogs = capturedLogs.filter(log => log.message.includes('Rapid'));
      expect(rapidLogs.length).toBe(10);
    });
  });

  describe('Network and Transport Simulation', () => {
    it('should handle structured network-style logs', async () => {
      const networkLogs: any[] = [];
      const logTypes = new Set<string>();
      const requestResponsePairs = new Map<number, { request?: any; response?: any }>();

      logMonitor.on('log-entry', (entry) => {
        try {
          // Try to parse as JSON
          const match = entry.message.match(/(\{.*\})/);
          if (match) {
            const logData = JSON.parse(match[1]);
            networkLogs.push(logData);
            
            if (logData.type) {
              logTypes.add(logData.type);
              
              // Track request/response pairs
              if (logData.type === 'http_request' && logData.requestId) {
                if (!requestResponsePairs.has(logData.requestId)) {
                  requestResponsePairs.set(logData.requestId, {});
                }
                requestResponsePairs.get(logData.requestId)!.request = logData;
              } else if (logData.type === 'http_response' && logData.requestId) {
                if (!requestResponsePairs.has(logData.requestId)) {
                  requestResponsePairs.set(logData.requestId, {});
                }
                requestResponsePairs.get(logData.requestId)!.response = logData;
              }
            }
          }
        } catch (e) {
          // Not JSON, ignore
        }
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'network-sim.js')}"`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-exited', (event) => {
          if (event.processId === processId) {
            resolve();
          }
        });
      });

      // Verify various network log types
      expect(logTypes.has('http_request')).toBe(true);
      expect(logTypes.has('http_response')).toBe(true);
      expect(logTypes.has('connection_opened')).toBe(true);
      expect(logTypes.has('connection_closed')).toBe(true);
      expect(logTypes.has('performance_metric')).toBe(true);

      // Verify request/response correlation
      const completePairs = Array.from(requestResponsePairs.values())
        .filter(pair => pair.request && pair.response);
      expect(completePairs.length).toBeGreaterThan(0);

      // Verify response timing
      completePairs.forEach(pair => {
        const requestTime = new Date(pair.request.timestamp).getTime();
        const responseTime = new Date(pair.response.timestamp).getTime();
        expect(responseTime).toBeGreaterThan(requestTime);
        expect(pair.response.duration).toBeDefined();
      });

      // Verify metric logs
      const metricLogs = networkLogs.filter(log => log.type === 'performance_metric');
      expect(metricLogs.length).toBeGreaterThan(0);
      metricLogs.forEach(metric => {
        expect(metric.metrics).toBeDefined();
        expect(metric.metrics.cpu).toBeDefined();
        expect(metric.metrics.memory).toBeDefined();
      });
    });
  });

  describe('Memory and Resource Management', () => {
    it('should monitor memory usage during leak simulation', async () => {
      const memorySnapshots: any[] = [];
      let initialHeap = 0;
      let finalHeap = 0;

      logMonitor.on('log-entry', (entry) => {
        try {
          const match = entry.message.match(/(\{.*\})/);
          if (match) {
            const logData = JSON.parse(match[1]);
            if (logData.type === 'memory_status') {
              memorySnapshots.push(logData);
              if (logData.iteration === 1) {
                initialHeap = logData.heapUsed;
              }
              finalHeap = logData.heapUsed;
            }
          }
        } catch (e) {
          // Not JSON, ignore
        }
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'memory-leak-sim.js')}" 8`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-exited', (event) => {
          if (event.processId === processId) {
            resolve();
          }
        });
      });

      // Verify memory monitoring
      expect(memorySnapshots.length).toBeGreaterThanOrEqual(8);
      
      // Verify memory increase (simulated leak)
      expect(finalHeap).toBeGreaterThan(initialHeap);
      
      // Verify progressive memory growth
      for (let i = 1; i < memorySnapshots.length; i++) {
        expect(memorySnapshots[i].leakyArraysSize).toBeGreaterThan(
          memorySnapshots[i-1].leakyArraysSize
        );
      }

      // Verify memory tracking accuracy
      memorySnapshots.forEach(snapshot => {
        expect(snapshot.heapUsed).toBeGreaterThan(0);
        expect(snapshot.heapTotal).toBeGreaterThanOrEqual(snapshot.heapUsed);
        expect(snapshot.rss).toBeGreaterThanOrEqual(snapshot.heapTotal);
      });
    });
  });

  describe('Error Handling and Recovery', () => {
    it('should capture cascading errors with full context', async () => {
      const errorLogs: any[] = [];
      const errorLevels = new Set<string>();
      const shutdownSequence: string[] = [];

      logMonitor.on('log-entry', (entry) => {
        if (entry.level === 'error' || entry.message.includes('[ERROR]') || 
            entry.message.includes('[FATAL]')) {
          errorLogs.push(entry);
        }
        
        // Track error cascade
        if (entry.message.includes('Level 1 error')) errorLevels.add('level1');
        if (entry.message.includes('Level 2 error')) errorLevels.add('level2');
        if (entry.message.includes('Level 3 error')) errorLevels.add('level3');
        
        // Track shutdown sequence
        if (entry.message.includes('Attempting graceful shutdown')) {
          shutdownSequence.push('shutdown_start');
        } else if (entry.message.includes('Closing database')) {
          shutdownSequence.push('close_db');
        } else if (entry.message.includes('Flushing buffers')) {
          shutdownSequence.push('flush_buffers');
        } else if (entry.message.includes('Saving state')) {
          shutdownSequence.push('save_state');
        } else if (entry.message.includes('Shutdown In Progress')) {
          shutdownSequence.push('shutdown_complete');
        }
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'error-cascade.js')}"`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-crashed', (event) => {
          if (event.processId === processId) {
            expect(event.code).toBe(1);
            resolve();
          }
        });
      });

      // Verify error cascade was captured
      expect(errorLevels.has('level1')).toBe(true);
      expect(errorLevels.has('level2')).toBe(true);
      expect(errorLevels.has('level3')).toBe(true);

      // Verify error propagation order
      const level3Index = errorLogs.findIndex(log => log.message.includes('Level 3 error'));
      const level2Index = errorLogs.findIndex(log => log.message.includes('Level 2 error'));
      const level1Index = errorLogs.findIndex(log => log.message.includes('Level 1 error'));
      
      expect(level3Index).toBeLessThan(level2Index);
      expect(level2Index).toBeLessThan(level1Index);

      // Verify shutdown sequence
      expect(shutdownSequence).toEqual([
        'shutdown_start',
        'close_db',
        'flush_buffers',
        'save_state',
        'shutdown_complete'
      ]);

      // Verify stack traces were captured
      const stackTraceLogs = errorLogs.filter(log => 
        log.message.includes('Stack trace') || log.message.includes('at ')
      );
      expect(stackTraceLogs.length).toBeGreaterThan(0);
    });
  });

  describe('Advanced Stream Processing', () => {
    it('should handle custom stream processing with filters', async () => {
      // Create a custom process with pipes
      const child = spawn('node', ['-e', `
        const interval = setInterval(() => {
          const level = ['INFO', 'WARN', 'ERROR', 'DEBUG'][Math.floor(Math.random() * 4)];
          const message = 'Custom stream test ' + Date.now();
          console.log('[' + level + '] ' + message);
        }, 50);
        
        setTimeout(() => {
          clearInterval(interval);
          process.exit(0);
        }, 2000);
      `]);

      // Create enhanced log stream with filtering
      const logStream = new EnhancedLogStream(child.stdout!, child.stderr!);
      const capturedLogs: any[] = [];
      const filteredLogs: any[] = [];

      // Capture all logs
      logStream.on('log-entry', (entry) => {
        capturedLogs.push(entry);
      });

      // Apply filter after 500ms
      setTimeout(() => {
        logStream.setLogLevelFilter(['error', 'warn']);
        
        // Capture filtered logs
        logStream.on('log-entry', (entry) => {
          filteredLogs.push(entry);
        });
      }, 500);

      // Wait for process to complete
      await new Promise<void>((resolve) => {
        child.on('exit', () => {
          logStream.cleanup();
          resolve();
        });
      });

      // Verify stream processing
      expect(capturedLogs.length).toBeGreaterThan(20);
      
      // Verify filter configuration
      expect(logStream.isFilterActive()).toBe(true);
      expect(logStream.getFilterConfiguration()).toEqual(['error', 'warn']);

      // Note: Due to event listener timing, we verify the filter was applied
      // rather than checking filtered logs count
    });

    it('should handle stream errors gracefully', async () => {
      const streamErrors: any[] = [];
      
      // Create a process that will have stream issues
      const child = spawn('node', ['-e', `
        // Write to stdout
        console.log('[INFO] Starting stream error test');
        
        // Force close stdout
        process.stdout.destroy();
        
        // Try to write after destroy
        setTimeout(() => {
          try {
            console.log('[ERROR] This should fail');
          } catch (e) {
            process.stderr.write('[ERROR] stdout write failed: ' + e.message + '\\n');
          }
        }, 100);
        
        setTimeout(() => {
          process.stderr.write('[INFO] Test In Progress\\n');
          process.exit(0);
        }, 500);
      `]);

      const logStream = new LogStream(child.stdout!, child.stderr!);
      
      logStream.on('stream-error', (error) => {
        streamErrors.push(error);
      });

      const logs: any[] = [];
      logStream.on('log-entry', (entry) => {
        logs.push(entry);
      });

      await new Promise<void>((resolve) => {
        child.on('exit', () => {
          logStream.cleanup();
          resolve();
        });
      });

      // Verify error handling
      expect(logs.length).toBeGreaterThan(0);
      
      // Should have captured the stderr output
      const stderrLogs = logs.filter(log => log.source === 'stderr');
      expect(stderrLogs.length).toBeGreaterThan(0);
    });
  });

  describe('Multi-Process Coordination', () => {
    it('should handle parallel worker processes', async () => {
      const workerLogs = new Map<number, any[]>();
      const aggregator = new LogAggregator();

      logMonitor.on('log-entry', (entry) => {
        // Parse worker ID from message
        const match = entry.message.match(/\[WORKER-(\d+)\]/);
        if (match) {
          const workerId = parseInt(match[1]);
          if (!workerLogs.has(workerId)) {
            workerLogs.set(workerId, []);
          }
          workerLogs.get(workerId)!.push(entry);
        }
        
        aggregator.addLog(entry.processId, entry);
      });

      const processId = await logMonitor.startRealTimeMonitoring(
        `node "${path.join(testAppsDir, 'parallel-worker.js')}" 4`
      );

      await new Promise<void>((resolve) => {
        logMonitor.on('process-exited', (event) => {
          if (event.processId === processId) {
            resolve();
          }
        });
      });

      // Verify worker coordination
      expect(workerLogs.size).toBeGreaterThanOrEqual(4);
      
      // Each worker should have completed its tasks
      workerLogs.forEach((logs, workerId) => {
        const startLog = logs.find(log => log.message.includes('started with'));
        const completeLog = logs.find(log => log.message.includes('All tasks completed'));
        
        expect(startLog).toBeDefined();
        expect(completeLog).toBeDefined();
        
        // Verify task completion logs
        const taskLogs = logs.filter(log => log.message.includes('Task') && log.message.includes('In Progress'));
        expect(taskLogs.length).toBeGreaterThanOrEqual(5);
      });

      // Verify aggregation
      const stats = aggregator.getStatistics();
      expect(stats.totalLogs).toBeGreaterThan(20); // 4 workers * 5+ logs each
    });
  });

  describe('Performance Under Load', () => {
    it('should maintain performance with high-frequency structured logs', async () => {
      const performanceMetrics = {
        startTime: Date.now(),
        firstLogTime: 0,
        lastLogTime: 0,
        logCount: 0,
        parseErrors: 0
      };

      logMonitor.on('log-entry', (entry) => {
        performanceMetrics.logCount++;
        
        if (performanceMetrics.firstLogTime === 0) {
          performanceMetrics.firstLogTime = Date.now();
        }
        performanceMetrics.lastLogTime = Date.now();
        
        // Try to parse structured logs
        try {
          const match = entry.message.match(/(\{.*\})/);
          if (match) {
            JSON.parse(match[1]);
          }
        } catch (e) {
          performanceMetrics.parseErrors++;
        }
      });

      // Run multiple network simulators concurrently
      const processIds = await Promise.all([
        logMonitor.startRealTimeMonitoring(`node "${path.join(testAppsDir, 'network-sim.js')}"`),
        logMonitor.startRealTimeMonitoring(`node "${path.join(testAppsDir, 'network-sim.js')}"`),
        logMonitor.startRealTimeMonitoring(`node "${path.join(testAppsDir, 'network-sim.js')}"`)
      ]);

      await new Promise<void>((resolve) => {
        let completed = 0;
        logMonitor.on('process-exited', () => {
          completed++;
          if (completed === 3) {
            resolve();
          }
        });
      });

      const totalDuration = performanceMetrics.lastLogTime - performanceMetrics.startTime;
      const processingDuration = performanceMetrics.lastLogTime - performanceMetrics.firstLogTime;
      const throughput = performanceMetrics.logCount / (processingDuration / 1000);

      // Verify performance
      expect(performanceMetrics.logCount).toBeGreaterThan(60); // 3 processes * 20+ logs
      expect(throughput).toBeGreaterThan(30); // At least 30 logs/second
      expect(performanceMetrics.parseErrors).toBeLessThan(5); // Minimal parse errors
      
      // Verify low overhead
      const overhead = totalDuration - processingDuration;
      expect(overhead).toBeLessThan(500); // Less than 500ms startup overhead
    });
  });
});