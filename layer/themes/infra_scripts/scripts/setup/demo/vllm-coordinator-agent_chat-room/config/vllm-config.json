{
  "models": {
    "deepseek-r1": {
      "32b": {
        "name": "deepseek-ai/DeepSeek-R1-32B",
        "description": "DeepSeek R1 32B parameter model with advanced reasoning",
        "requirements": {
          "gpu_memory": 24,
          "ram": 32,
          "disk_space": 65
        },
        "parameters": {
          "temperature": 0.7,
          "top_p": 0.95,
          "top_k": 50,
          "max_tokens": 2048,
          "context_length": 32768
        }
      },
      "8b": {
        "name": "deepseek-ai/DeepSeek-R1-8B",
        "description": "DeepSeek R1 8B parameter model - faster, lighter version",
        "requirements": {
          "gpu_memory": 8,
          "ram": 16,
          "disk_space": 16
        },
        "parameters": {
          "temperature": 0.7,
          "top_p": 0.95,
          "top_k": 50,
          "max_tokens": 1024,
          "context_length": 16384
        }
      },
      "distill": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-32B",
        "description": "DeepSeek R1 Distilled version for balanced performance",
        "requirements": {
          "gpu_memory": 16,
          "ram": 24,
          "disk_space": 40
        },
        "parameters": {
          "temperature": 0.5,
          "top_p": 0.95,
          "top_k": 40,
          "max_tokens": 1536,
          "context_length": 32768
        }
      }
    }
  },
  "server": {
    "default_port": 8000,
    "gpu_memory_utilization": 0.9,
    "tensor_parallel_size": 1,
    "download_dir": "~/.vllm/models"
  },
  "chat": {
    "system_prompts": {
      "default": "You are DeepSeek-R1, a helpful AI assistant in a chat room. You should be friendly, helpful, and conversational.",
      "coding": "You are DeepSeek-R1, an expert programmer. Provide clean, well-commented code with explanations.",
      "reasoning": "You are DeepSeek-R1. Use your advanced reasoning capabilities to solve problems step by step."
    }
  }
}